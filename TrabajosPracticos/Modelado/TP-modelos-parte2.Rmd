---
title: "TP Modelos parte 2"
author: "Guillermo Solovey"
date: "10/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### 1. Tres elementos de la tarea de modelado

- representación
- evaluación 
- optimización

Usando el dataset `penguins` del paquete `palmerpenguins`queremos crear un modelo 
que permita estimar el largo del pico conociendo el peso de los pinguinos. En principio
vamos a trabajar sólo con los pinguinos de la especie Gentoo.

Antes de empezar a construir el modelo, grafiquen! Hagan un scatter plot del largo del 
pico en función del peso para los pinguinos Gentoo.

```{r include=FALSE}
# variable que si es TRUE muestra los chunks con la solucion
# a los ejercicios.
include.sol = F
eval.sol = F
```


```{r eval=eval.sol, message=FALSE, include=include.sol}
require(tidyverse)
require(palmerpenguins)

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm)) + 
  geom_point() +
  theme_minimal()

```

El primer paso es la representación o especificación del modelo. Es decir, definir 
cómo se relacionan las variables largo de pico y peso según el modelo. En este caso
vamos a usar una función lineal. ¿Tiene sentido proponer este modelo de acuerdo
a cómo se ven los datos en el gráfico que hicieron?

Aunque este problema se puede resolver con la función ´lm()´, entre otras, en este 
caso vamos a hacer explícitos los tres pasos que requiere el modelado.

**a**- Crear una función que tenga como inputs la ordenada al origen, la pendiente y el 
peso, y como output el largo del pico estimado por el modelo.
```{r eval=eval.sol, message=FALSE, include=include.sol}

fun_modelo <- function(a, b, x){
  y <- a + b * x
  return(y)
}
```


**b**- El siguiente paso es la evaluación. Es decir, dado un modelo con sus parámetros, 
queremos conocer una medida del ajuste del modelos a los datos. Para eso vamos a crear 
una función que tenga como inputs la variable a explicar (largo del pico) y las
predicciones del modelo. La llamamos función de pérdida (`fun_loss`) y en este caso
queremos que devuela la raiz cuadrada del error cuadrático medio (*RMSE*). Noten que
la función de pérdida podría ser otra.

```{r eval=eval.sol, message=FALSE, include=include.sol}

fun_loss <- function (ypred, y){
  L <- sqrt( mean( (y - ypred)^2 ) )
  return(L)
}
```

**c**- Por último, la optimización. Es decir, cómo vamos a encontrar los parámetros que 
minimizan la función de pérdida. En este caso vamos a usar un método de optimización
aleatoria. Es decir, queremos una función que:

* defina valores iniciales de parámetros `(a,b)` y un rango inicial de cada parámetro.
* evalúe el *RMSE* inicial.
* sortee nuevos parámetros dentro de la región definida y evalue el *RMSE* para estos
nuevos parametros. 
* si el *RMSE* disminuye, tomo los nuevos valores de `(a,b)` y si no achico la region
de busqueda.
* Cuando la region de busqueda es lo suficientemente chica, termina y devuelve los 
parametros finales.

Mas detalles en el [apunte](https://guillermosolovey.github.io/laboratoriodedatos2021-2C/NotebooksTeoricas/Modelado/Modelado_con_Regresion_Lineal_Multiple_Final.pdf) y [videos](https://youtube.com/playlist?list=PLN2e9R_DoC0THQgbage3DkS_7tCsqTd-3) de Andres Farall. El factor de reduccion y la 
tolerancia pueden estar definidas por defecto en la funcion. Por ejemplo, ´0.95´ y ´1e-10´
respectivamente. Tenga en cuenta que esta función tiene que llamar a las otras dos
funciones creadas anteriormente.

```{r eval=eval.sol, message=FALSE, include=include.sol}

# valores iniciales de a y b
a.ini <- 26
b.ini <- 0
a.rango.ini <- 1
b.rango.ini <- 1

# factor de reducción de la ventana de busqueda (definidas por defecto igualmente)
f.red <- 0.95
f.red.tol <- 1e-10

fun_optimizacion <- function(x, y, 
                             a.ini, b.ini, 
                             a.rango.ini, b.rango.ini, 
                             f.red = 0.95, f.red.tol = 1e-10){
  
  # valores iniciales
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  ypred <- fun_modelo(a, b, x)
  L <- fun_loss(ypred, y)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    ypred <- fun_modelo(a.new, b.new, x)
    L.new <- fun_loss(ypred, y)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
  }
  
  p.opt = data.frame(a, b)
  return(p.opt)
}
```

**d**- Usando las 3 funciones creadas, armar un script que cargue los datos, 
ajuste el modelo, devuelva los parametros optimos y grafique los datos y 
la recta estimada por el modelo.

```{r eval=eval.sol, message=FALSE, include=include.sol}
require(tidyverse)
require(palmerpenguins)

#---------- cargo las funciones

fun_modelo <- function(a, b, x){
  y <- a + b * x
  return(y)
}

fun_loss <- function (ypred, y){
  L <- sqrt( mean( (y - ypred)^2 ) )
  return(L)
}

fun_optimizacion <- function(x, y, 
                             a.ini, b.ini, 
                             a.rango.ini, b.rango.ini, 
                             f.red = 0.95, f.red.tol = 1e-10){
  
  # valores iniciales
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  ypred <- fun_modelo(a, b, x)
  L <- fun_loss(ypred, y)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    ypred <- fun_modelo(a.new, b.new, x)
    L.new <- fun_loss(ypred, y)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
  }
  
  p.opt = data.frame(a, b)
  return(p.opt)
}

#---------- cargo los datos
d <- penguins %>% filter(species == "Gentoo") %>% drop_na() 
x <- d$body_mass_g
y <- d$bill_length_mm


#---------- optimizaciion

# valores iniciales para la opt
a.ini <- 26
b.ini <- 0
a.rango.ini <- 1
b.rango.ini <- 1

p.opt <- fun_optimizacion(x, y, a.ini, b.ini, a.rango.ini, b.rango.ini)

# parametros optimos
print(p.opt)

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm)) + 
  geom_point() +
  geom_abline(intercept = p.opt$a, slope = p.opt$b) + 
  theme_minimal() 

```

**e**- ¿Les dio una recta razonable? ¿Cómo están seguros de que el método está convergiendo? Repitan el proceso
anterior pero modifiquen las funciones para guardar en cada iteración aleatoria el valor de los parámetros
(ordenada al origen y pendiente) y la función de pérdida. Grafiquen. Vean qué pasa si empiezan muy lejos
de los valores óptimos. Comparen, por ejemplo, empezar la búsqueda por $y=1+0~x$ o con $y=26+0~x$. 
¿LLegan al mismo resultado?

```{r eval=eval.sol, message=FALSE, include=include.sol}
require(tidyverse)
require(palmerpenguins)

#---------- cargo las funciones

fun_modelo <- function(a, b, x){
  y <- a + b * x
  return(y)
}

fun_loss <- function (ypred, y){
  L <- sqrt( mean( (y - ypred)^2 ) )
  return(L)
}

fun_optimizacion <- function(x, y, 
                             a.ini, b.ini, 
                             a.rango.ini, b.rango.ini, 
                             f.red = 0.99, f.red.tol = 1e-10){
  
  params.a <- {}
  params.b <- {}
  params.L <- {}
  
  # valores iniciales
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  ypred <- fun_modelo(a, b, x)
  L <- fun_loss(ypred, y)

  params.a[1] <- a
  params.b[1] <- b
  params.L[1] <- L

  # busco parametros mejores
  f.red.actual <- 1
  k <- 2
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    ypred <- fun_modelo(a.new, b.new, x)
    L.new <- fun_loss(ypred, y)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
   
    
  params.a[k] <- a
  params.b[k] <- b
  params.L[k] <- L
  
  k <- k + 1
  }
  
  out <- data.frame(params.a, params.b, params.L)
  return(out)
}

#---------- cargo los datos
d <- penguins %>% filter(species == "Gentoo") %>% drop_na() 
x <- d$body_mass_g
y <- d$bill_length_mm


#---------- optimizaciion

# caso 1
a.ini <- 1
b.ini <- 0
a.rango.ini <- 80
b.rango.ini <- 1
out1 <- fun_optimizacion(x, y, a.ini, b.ini, a.rango.ini, b.rango.ini)

# caso 2: no llega al mismo resultado. mínimo local?
a.ini <- 26
b.ini <- 0
a.rango.ini <- 80
b.rango.ini <- 1
out2 <- fun_optimizacion(x, y, a.ini, b.ini, a.rango.ini, b.rango.ini)


penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm)) + 
  geom_point() +
  geom_abline(intercept = tail(out1$params.a,1), slope = tail(out1$params.b,1)) + 
  geom_abline(intercept = tail(out2$params.a,1), slope = tail(out2$params.b,1)) + 
  theme_minimal() 

```

**f**- Otro test del proceso de optimización.
Si repiten el proceso de optimización aleatoria empezando con los mismos valores iniciales, 
¿llegan a la misma solución? Una propiedad importante
de un método de optimización es que los parámetros óptimos que encuentra
el método no sean casuales Para estudiar esto, repitan el proceso de optimización 100 veces y hagan una
descripción de los parámetros estimados. Pueden hacer un histograma de `a` y de `b`. Estudien 
nuevamente si
los valores estimados dependen de los valores iniciales para  `a` y `b`.


**g**- ¿Cuál es el método de optimización que usa la función `lm`? Comparen los parámetros estimados con el 
método de optimización aleatoria en f- con los que se obtienen con la función `lm`. Si el método funciona
bien, un histograma de los parámetros estimados con el método de optimización aleatoria debería incluir
a los valores estimados con la función `lm`.

```{r eval=eval.sol, message=FALSE, include=include.sol}

#---------- solución puntos f y g
require(tidyverse)
require(palmerpenguins)

#---------- cargo las funciones

fun_modelo <- function(a, b, x){
  y <- a + b * x
  return(y)
}

fun_loss <- function (ypred, y){
  L <- sqrt( mean( (y - ypred)^2 ) )
  return(L)
}

fun_optimizacion <- function(x, y, 
                             a.ini, b.ini, 
                             a.rango.ini, b.rango.ini, 
                             f.red = 0.95, f.red.tol = 1e-10){
  
  # valores iniciales
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  ypred <- fun_modelo(a, b, x)
  L <- fun_loss(ypred, y)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    ypred <- fun_modelo(a.new, b.new, x)
    L.new <- fun_loss(ypred, y)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
  }
  
  p.opt = data.frame(a, b)
  return(p.opt)
}


#---------- cargo los datos
d <- penguins %>% filter(species == "Gentoo") %>% drop_na() 
x <- d$body_mass_g
y <- d$bill_length_mm


# valores iniciales para la opt
a.ini <- 26
b.ini <- 0
a.rango.ini <- 1
b.rango.ini <- 1

N = 500
p = data.frame(a = rep(NA, N), b = rep(NA, N))

for (i in 1:N){
  pp <- fun_optimizacion(x, y, a.ini, b.ini, a.rango.ini, b.rango.ini)
  p$a[i] <- pp$a
  p$b[i] <- pp$b
}


lm.coef = coef( lm(bill_length_mm ~ body_mass_g, data = d) )

p <- p %>% pivot_longer(cols = 1:2, names_to = "variable")

ggplot(data = p, mapping = aes(x = value, fill = variable)) + 
  geom_histogram(alpha=0.5) +
  facet_wrap(~variable, scales = "free") +
  geom_vline(data = filter(p, variable=="a"), aes(xintercept = lm.coef[1]), size = 2) + 
  geom_vline(data = filter(p, variable=="b"), aes(xintercept = lm.coef[2]), size = 2) +
  theme_bw() + 
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

```

**h**- Cambien la función de pérdida por otra que se les ocurra y estudien si 
los parámetros estimados son compatibles con los que obtuvieron usando como función de pérdida a la
raíz cuadrada del error cuadrático medio.

```{r eval=eval.sol, message=FALSE, include=include.sol}
require(tidyverse)
require(palmerpenguins)

#---------- cargo las funciones

fun_modelo <- function(a, b, x){
  y <- a + b * x
  return(y)
}

fun_loss <- function (ypred, y){
  L <- log(mean( abs(y - ypred) ))
  return(L)
}

fun_optimizacion <- function(x, y, 
                             a.ini, b.ini, 
                             a.rango.ini, b.rango.ini, 
                             f.red = 0.95, f.red.tol = 1e-10){
  
  # valores iniciales
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  ypred <- fun_modelo(a, b, x)
  L <- fun_loss(ypred, y)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    ypred <- fun_modelo(a.new, b.new, x)
    L.new <- fun_loss(ypred, y)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
  }
  
  p.opt = data.frame(a, b)
  return(p.opt)
}

#---------- cargo los datos
d <- penguins %>% filter(species == "Gentoo") %>% drop_na() 
x <- d$body_mass_g
y <- d$bill_length_mm


#---------- optimizaciion

# valores iniciales para la opt
a.ini <- 26
b.ini <- 0
a.rango.ini <- 1
b.rango.ini <- 1

N = 500
p = data.frame(a = rep(NA, N), b = rep(NA, N))

for (i in 1:N){
  p[i,] <- fun_optimizacion(x, y, a.ini, b.ini, a.rango.ini, b.rango.ini)
}


lm.coef = coef( lm(bill_length_mm ~ body_mass_g, data = d) )

p <- p %>% pivot_longer(cols = 1:2, names_to = "variable")

ggplot(data = p, mapping = aes(x = value, fill = variable)) + 
  geom_histogram(alpha=0.5) +
  facet_wrap(~variable, scales = "free") +
  geom_vline(data = filter(p, variable=="a"), aes(xintercept = lm.coef[1]), size = 2) + 
  geom_vline(data = filter(p, variable=="b"), aes(xintercept = lm.coef[2]), size = 2) +
  theme_bw() + 
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

```

##### 2. Los 3 elementos del modelado en un problema diferente

El objetivo de este ejercicio es crear los 3 elementos de la tarea de modelado 
para reconocer el sexo de los pinguinos de la especie Gentoo conociendo sólo
la información del peso.

La idea es encontrar una forma de separar a los pinguinos masculinos y
femeninos. Vamos a usar lo siguiente:

- representación: si peso > p0 es masculino. si no, es femenino.
- evaluación: error de clasificación (# de pinguinos clasificados mal)
- optimización: optimización aleatoria. 

Por ejemplo, en la figura de abajo se muestra un ejemplo usando como peso
de corte p0 = 5300 gramos.

```{r echo=FALSE, message=FALSE, warning=FALSE}

require(tidyverse)
require(palmerpenguins)

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, fill=sex, color=sex)) + 
  geom_histogram(position="identity", alpha=0.5) +
  geom_vline(xintercept = 5300) + 
  theme_minimal()


```

**a**- Ustedes tienen que encontrar cuál es el mejor peso de corte p0 y cuál es el error 
para ese criterio adaptando lo que hicieron en el ejercicio 1 a esta nueva situación.

```{r eval=eval.sol, message=FALSE, include=include.sol}

#---------- cargo las funciones

# representacion
fun_modelo <- function(p0, x){
  c <- ifelse(x>p0,"male","female")
  return(c)
}

# evalucacion
fun_loss <- function (cpred, c){
  L <- mean( c != cpred ) 
  return(L)
}

# optimizacion
fun_optimizacion <- function(x, c,
                             p0.ini,
                             p0.rango.ini,
                             f.red = 0.95, f.red.tol = 1e-20){
  
  # valor inicial
  p0 <- runif(1, min = p0.ini-p0.rango.ini/2, max = p0.ini+p0.rango.ini/2)
  
  # predicciones y loss
  cpred <- fun_modelo(p0, x)
  L <- fun_loss(cpred, c)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    p0.new <- runif(1, 
                    min = p0-(p0.rango.ini*f.red.actual)/2, 
                    max = p0+(p0.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    cpred <- fun_modelo(p0.new, x)
    L.new <- fun_loss(cpred, c)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L  <- L.new
      p0 <- p0.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
    
  }
  
  return(p0)
}

#---------- cargo los datos
d <- penguins %>% filter(species == "Gentoo") %>% drop_na() 
x <- d$body_mass_g
c <- d$sex


#---------- optimizacion

# valores iniciales para la opt
p0.ini <- 4500
p0.rango.ini <- 1000

p0.opt <- rep(NA,100)
for (i in 1:100){
  p0.opt[i] <- fun_optimizacion(x, c, p0.ini, p0.rango.ini)
}

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, fill=sex, color=sex)) + 
  geom_histogram(position="identity", alpha=0.5) +
  geom_vline(xintercept = summary(p0.opt)[2]) + 
  geom_vline(xintercept = summary(p0.opt)[5]) + 
  theme_minimal()

# error con el criterio optimo
fun_loss(cpred = fun_modelo(median(p0.opt), d$body_mass_g), c = d$sex)

```



**b**- Ahora consideren que pueden usar también el largo del pico. Una posibilidad es dividir al plano 
con una recta como la de la figura de abajo.

```{r echo=FALSE, message=FALSE, warning=FALSE}
require(tidyverse)
require(palmerpenguins)

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm, color=sex)) + 
  geom_point() +
  geom_abline(intercept = 62, slope = -0.003) +
  theme_minimal()
```

El ejercicio es reemplazar el modelo anterior que usaba sólo el peso por uno que usa también
el largo del pico. Es decir:

- representación: si `y > a + b x` es masculino. si no, es femenino.
- evaluación: error de clasificación (# de pinguinos clasificados mal)
- optimización: optimización aleatoria. 


**c**- ¿Cuánto mejora la clasificación ahora que incluyeron esta otra variable? 




```{r eval=eval.sol, message=FALSE, include=include.sol}

#---------- cargo las funciones

# representacion
fun_modelo <- function(a, b, x, y){
  cpred <- ifelse( y > (a + b * x), "male", "female")
  return(cpred)
}

# evalucacion
fun_loss <- function (cpred, c){
  L <- mean( c != cpred ) 
  return(L)
}

# optimizacion de a y b
fun_optimizacion <- function(x, y, c,
                             a.ini, b.ini,
                             a.rango.ini, b.rango.ini,
                             f.red = 0.99, f.red.tol = 1e-20){
  
  # valor inicial
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  cpred <- fun_modelo(a, b, x, y)
  L <- fun_loss(cpred, c)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    cpred <- fun_modelo(a, b, x, y)
    L.new <- fun_loss(cpred, c)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
    
  }
  
  return(c(a,b))
}

#---------- cargo los datos
d <- penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  mutate(x = (body_mass_g - mean(body_mass_g)) / sd(body_mass_g),
         y = (bill_length_mm - mean(bill_length_mm)) / sd(bill_length_mm)) 

#---------- optimizacion

# valores iniciales para la opt
a.ini <- 0
a.rango.ini <- 20
b.ini <- -2
b.rango.ini <- 20

p.opt <- data.frame(a = rep(NA,100), b = rep(NA,100))
for (i in 1:100){
  p.opt[i,] <- fun_optimizacion(d$x, d$y, d$sex, a.ini, b.ini, a.rango.ini, b.rango.ini)
}


penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(data = d, mapping = aes(x, y, color=sex)) + 
  geom_point() +
  geom_abline(intercept = median(p.opt$a), slope = median(p.opt$b), colour = "violet") +
  geom_abline(intercept = median(p.opt$a), slope = -60) +
  theme_minimal()


# error con el criterio optimo
fun_loss(cpred = fun_modelo(median(p.opt$a), median(p.opt$b), d$x, d$y), c = d$sex)

# error usando otras rectas más verticales... parece bajar
fun_loss(cpred = fun_modelo(median(p.opt$a), -40, d$x, d$y), c = d$sex)
fun_loss(cpred = fun_modelo(-10, -60, d$x, d$y), c = d$sex)


# pruebo con otra funcion de optimizacion - grilla
# 
# a.min = -10
# a.max = +10
# b.min = -10
# b.max = +10
# 
# fun_optimizacion2(d$x, d$y, d$sex, a.ini, b.min, a.max, b.max)
# 
# fun_optimizacion2 <- function(x, y, c,
#                              a.min, b.min,
#                              a.max, b.max){
#   
#   a <- seq(a.min,a.max,length.out=500)
#   b <- seq(b.min,b.max,length.out=500)
#   L <- matrix(nrow = 500, ncol = 500)
#   for (i in 1:500){
#     for (j in 1:500){
#       
#       cpred <- fun_modelo(a[i], b[j], x, y)
#       L[i,j] <- fun_loss(cpred, c)
#       
#     }
#   }
#   
#   # falta corregir el output de esta función:
#   return(c(min(L), which(L==min(L), arr.ind = T)))
# }
```


**d** ¿Qué pasaría si la recta
óptima es una recta vertical? ¿Cómo podrían modificar el modelo para que incluya también el caso de 
una recta vertical?

