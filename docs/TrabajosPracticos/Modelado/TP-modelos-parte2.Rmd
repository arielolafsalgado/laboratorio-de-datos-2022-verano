---
title: "TP Modelos parte 2"
author: "Guillermo Solovey"
date: "10/27/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

##### 1. Tres elementos de la tarea de modelado

- representación
- evaluación 
- optimización

Usando el dataset `penguins` del paquete `palmerpenguins`queremos crear un modelo 
que permita estimar el largo del pico conociendo el peso de los pinguinos. En principio
vamos a trabajar sólo con los pinguinos de la especie Gentoo.

Antes de empezar a construir el modelo, grafiquen! Hagan un scatter plot del largo del 
pico en función del peso para los pinguinos Gentoo.
```{r eval=FALSE, message=FALSE, include=FALSE}
require(tidyverse)
require(palmerpenguins)

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm)) + 
  geom_point() +
  theme_minimal()

```

El primer paso es la representación o especificación del modelo. Es decir, definir 
cómo se relacionan las variables largo de pico y peso según el modelo. En este caso
vamos a usar una función lineal. ¿Tiene sentido proponer este modelo de acuerdo
a cómo se ven los datos en el gráfico que hicieron?

Aunque este problema se puede resolver con la función ´lm()´, entre otras, en este 
caso vamos a hacer explícitos los tres pasos que requiere el modelado.

a- Crear una función que tenga como inputs la ordenada al origen, la pendiente y el 
peso, y como output el largo del pico.
```{r eval=FALSE, message=FALSE, include=FALSE}

fun_modelo <- function(a, b, x){
  y <- a + b * x
  return(y)
}
```


b- El siguiente paso es la evaluación. Es decir, dado un modelo con sus parámetros, 
queremos conocer una medida del ajuste del modelos a los datos. Para eso vamos a crear 
una función que tenga como inputs la variable a explicar (largo del pico) y las
predicciones del modelo. La llamamos función de pérdida (`fun_loss`) y en este caso
queremos que devuela la raiz cuadrada del error cuadrático medio (*RMSE*). Noten que
la función de pérdida podría ser otra.

```{r eval=FALSE, message=FALSE, include=FALSE}

fun_loss <- function (ypred, y){
  L <- sqrt( mean( (y - ypred)^2 ) )
  return(L)
}
```

c- Por último, la optimización. Es decir, cómo vamos a encontrar los parámetros que 
minimizan la función de pérdida. En este caso vamos a usar un método de optimización
aleatoria. Es decir, queremos una función que:
* defina valores iniciales de parámetros `(a,b)` y un rango inicial de cada parámetro.
* evalúe el *RMSE* inicial.
* sortee nuevos parámetros dentro de la región definida y evalue el *RMSE* para estos
nuevos parametros. 
* si el *RMSE* disminuye, tomo los nuevos valores de `(a,b)` y si no achico la region
de busqueda.
* Cuando la region de busqueda es lo suficientemente chica, termina y devuelve los 
parametros finales.

Mas detalles en el [apunte](https://guillermosolovey.github.io/laboratoriodedatos2021-2C/NotebooksTeoricas/Modelado/Modelado_con_Regresion_Lineal_Multiple_Final.pdf) y [videos](https://youtube.com/playlist?list=PLN2e9R_DoC0THQgbage3DkS_7tCsqTd-3) de Andres Farall. El factor de reduccion y la 
tolerancia pueden estar definidas por defecto en la funcion. Por ejemplo, ´0.95´ y ´1e-10´
respectivamente. Tenga en cuenta que esta función tiene que llamar a las otras dos
funciones creadas anteriormente.

```{r eval=FALSE, message=FALSE, include=FALSE}

# valores iniciales de a y b
a.ini <- 26
b.ini <- 0
a.rango.ini <- 1
b.rango.ini <- 1

# factor de reducción de la ventana de busqueda (definidas por defecto igualmente)
f.red <- 0.95
f.red.tol <- 1e-10

fun_optimizacion <- function(x, y, 
                             a.ini, b.ini, 
                             a.rango.ini, b.rango.ini, 
                             f.red = 0.95, f.red.tol = 1e-10){
  
  # valores iniciales
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  ypred <- fun_modelo(a, b, x)
  L <- fun_loss(ypred, y)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    ypred <- fun_modelo(a.new, b.new, x)
    L.new <- fun_loss(ypred, y)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
  }
  
  p.opt = data.frame(a, b)
  return(p.opt)
}
```

d- Usando las 3 funciones creadas, armar un script que cargue los datos, 
ajuste el modelo, devuelva los parametros optimos y grafique los datos y 
la recta estimada por el modelo.

```{r eval=FALSE, message=FALSE, include=FALSE}

#---------- Solución al ej 1d


require(tidyverse)
require(palmerpenguins)

#---------- cargo las funciones

fun_modelo <- function(a, b, x){
  y <- a + b * x
  return(y)
}

fun_loss <- function (ypred, y){
  L <- sqrt( mean( (y - ypred)^2 ) )
  return(L)
}

fun_optimizacion <- function(x, y, 
                             a.ini, b.ini, 
                             a.rango.ini, b.rango.ini, 
                             f.red = 0.95, f.red.tol = 1e-10){
  
  # valores iniciales
  a <- runif(1, min = a.ini-a.rango.ini/2, max = a.ini+a.rango.ini/2)
  b <- runif(1, min = b.ini-b.rango.ini/2, max = b.ini+b.rango.ini/2)
  
  # predicciones y loss
  ypred <- fun_modelo(a, b, x)
  L <- fun_loss(ypred, y)
  
  # busco parametros mejores
  f.red.actual <- 1
  while( f.red.actual > f.red.tol ){
    
    # genero nuevos parametros
    a.new <- runif(1, 
                   min = a-(a.rango.ini*f.red.actual)/2, 
                   max = a+(a.rango.ini*f.red.actual)/2)
    b.new <- runif(1, 
                   min = b-(b.rango.ini*f.red.actual)/2, 
                   max = b+(b.rango.ini*f.red.actual)/2)
    
    # predicciones y loss con el modelo m1 y nuevos params
    ypred <- fun_modelo(a.new, b.new, x)
    L.new <- fun_loss(ypred, y)
    
    if (L.new < L){
      # si el ajuste es mejor con los nuevos valores me
      # quedo con los nuevos
      L <- L.new
      a <- a.new
      b <- b.new
    }
    else{
      # si no lo es, me quedo con los "viejos" y busco en 
      # una región más pequeña.
      f.red.actual <- f.red.actual * f.red
    }
    
  }
  
  p.opt = data.frame(a, b)
  return(p.opt)
}

#---------- cargo los datos
d <- penguins %>% filter(species == "Gentoo") %>% drop_na() 
x <- d$body_mass_g
y <- d$bill_length_mm


#---------- optimizaciion

# valores iniciales para la opt
a.ini <- 26
b.ini <- 0
a.rango.ini <- 1
b.rango.ini <- 1

p.opt <- fun_optimizacion(x, y, a.ini, b.ini, a.rango.ini, b.rango.ini)

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm)) + 
  geom_point() +
  geom_abline(intercept = p.opt$a, slope = p.opt$b) + 
  theme_minimal() 

```


e- Si repiten el proceso de optimización aleatoria, ¿llegan a la misma solución? Una propiedad importante
de un método de optimización es que sea robusto, en el sentido de que los parámetros óptimos que encuentra
el método no sean casualidad. Para estudiar esto, repitan el proceso de optimización 100 veces y hagan una
descripción de los parámetros estimados. Pueden hacer un histograma de `a` y de `b`. Estudien también si
los valores estimados dependen de los valores iniciales para  `a` y `b`.


f- ¿Cuál es el método de optimización que usa la función `lm`? Comparen los parámetros estimados con el 
método de optimización aleatoria en e- con los que se obtienen con la función `lm`. Si el método funciona
bien, un histograma de los parámetros estimados con el método de optimización aleatoria debería incluir
a los valores estimados con la función `lm`.

```{r eval=FALSE, message=FALSE, include=FALSE}


#---------- cargo los datos
d <- penguins %>% filter(species == "Gentoo") %>% drop_na() 
x <- d$body_mass_g
y <- d$bill_length_mm

#---------- cargo los datos
N = 100
p = data.frame(a = rep(NA, N), b = rep(NA, N))

# valores iniciales para la opt
a.ini <- 26
b.ini <- 0
a.rango.ini <- 1
b.rango.ini <- 1

for (i in 1:N){
  p[i,] <- fun_optimizacion(x, y, a.ini, b.ini, a.rango.ini, b.rango.ini)
}


lm.coef = coef( lm(bill_length_mm ~ body_mass_g, data = d) )

p <- p %>% pivot_longer(cols = 1:2, names_to = "variable")

ggplot(data = p, mapping = aes(x = value, fill = variable)) + 
  geom_histogram(alpha=0.5) +
  facet_wrap(~variable, scales = "free") +
  geom_vline(data = filter(p, variable=="a"), aes(xintercept = lm.coef[1]), size = 2) + 
  geom_vline(data = filter(p, variable=="b"), aes(xintercept = lm.coef[2]), size = 2) +
  theme_bw() + 
  theme(
    strip.background = element_blank(),
    strip.text.x = element_blank()
  )

```

f- Cambien la función de pérdida por una que calcule el promedio del error absoluto y estudien si 
los parámetros estimados son compatibles con los que obtuvieron usando como función de pérdida a la
raíz cuadrada del error cuadrático medio.


##### 2. Los 3 elementos del modelado en un problema diferente

El objetivo de este ejercicio es crear los 3 elementos de la tarea de modelado 
para reconocer el sexo de los pinguinos de la especie Gentoo conociendo sólo
la información del peso.

La idea es encontrar una forma de separar a los pinguinos masculinos y
femeninos. Vamos a usar lo siguiente:

- representación: si peso > p0 es masculino. si no, es femenino.
- evaluación: error de clasificación (# de pinguinos clasificados mal)
- optimización: optimización aleatoria. 

Por ejemplo, en la figura de abajo se muestra un ejemplo usando como peso
de corte p0 = 5300 gramos.

```{r echo=FALSE, message=FALSE, warning=FALSE}

require(tidyverse)
require(palmerpenguins)
# penguins %>% 
#   filter(species == "Gentoo") %>% 
#   drop_na() %>% 
#   ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm, color=sex)) + 
#   geom_point() +
#   geom_vline(xintercept = 5300) + 
#   theme_minimal()


penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, fill=sex, color=sex)) + 
  geom_histogram(position="identity", alpha=0.5) +
  geom_vline(xintercept = 5300) + 
  theme_minimal()


```

a- Ustedes tienen que encontrar cuál es el mejor peso de corte p0 y cuál es el error 
para ese criterio.

b- Ahora consideren que pueden usar también el largo del pico. Una posibilidad es dividir al plano 
con una recta como la de la figura de abajo

```{r echo=FALSE, message=FALSE, warning=FALSE}
require(tidyverse)
require(palmerpenguins)

penguins %>% 
  filter(species == "Gentoo") %>% 
  drop_na() %>% 
  ggplot(mapping = aes(x=body_mass_g, y=bill_length_mm, color=sex)) + 
  geom_point() +
  geom_abline(intercept = 62, slope = -0.003) +
  theme_minimal()
```

El ejercicio es montar reemplazar el modelo anterior que usaba sólo el peso por uno que usa también
el largo del pico. Es decir:

- representación: si `y > a + b x` es masculino. si no, es femenino.
- evaluación: error de clasificación (# de pinguinos clasificados mal)
- optimización: optimización aleatoria. 

c- ¿Cuánto mejora la clasificación ahora que incluyeron esta otra variable? ¿Qué pasaría si la recta
óptima es una recta vertical? ¿Cómo podrían modificar el modelo para que incluya también el caso de 
una recta vertical?




